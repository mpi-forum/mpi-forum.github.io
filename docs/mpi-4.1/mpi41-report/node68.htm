<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-pt2pt/pt2pt-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi4-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi41-report-html.idx -basedef mpi4defs.txt -o mpi41-report.tex mpi-report.tex 
-->
<title>Semantics of Point-to-Point Communication</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h1><span id="Node68">4.5. Semantics of Point-to-Point Communication</span></h1>
<a href="node67.htm#Node67"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node53.htm#Node53"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node69.htm#Node69"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node53.htm#Node53"> Point-to-Point Communication</a>
<b>Next: </b><a href="node69.htm#Node69"> Buffer Allocation and Usage</a>
<b>Previous: </b><a href="node67.htm#Node67"> Communication Modes</a>
<p>
  
  
<P> 
A valid <font face="sans-serif"> MPI</font> implementation guarantees certain general properties of  
point-to-point communication, which are described in this section.  
<P> 
<b> Order.</b>  
Messages are <b> nonovertaking</b>:  
If a sender sends two messages in succession to the same destination, and  
both match the same receive, then this operation cannot receive the  
second message if the first one is still pending.  
If a receiver posts two receives in succession, and both match the same  
message,  
then the second receive operation cannot be satisfied by this message, if the  
first one is still pending.  
This requirement facilitates matching of sends to receives.  
It guarantees that message-passing code is deterministic, if <font face="sans-serif"> MPI</font> processes are  
single-threaded and the wildcard <font face="sans-serif"> MPI_ANY_SOURCE</font> is not used in receives.  
(Some of the calls described later, such as <font face="sans-serif"> MPI_CANCEL</font> or  
<font face="sans-serif"> MPI_WAITANY</font>, are additional sources of nondeterminism.)  
<P> 
If an <font face="sans-serif"> MPI</font> process has a single thread of execution, then any two communication operations  
executed by this <font face="sans-serif"> MPI</font> process are <b> ordered</b>.  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
The <font face="sans-serif"> MPI</font> Forum believes the following paragraph is ambiguous and may clarify the meaning in a future version of the <font face="sans-serif"> MPI</font> Standard.  
 (<em> End of advice to users.</em>) <br> 
On the other hand, if the <font face="sans-serif"> MPI</font> process is  
multithreaded, then the semantics of thread execution may not define  
a relative order between two send operations executed by two  
distinct threads.  The operations are <b> logically concurrent</b>, even if one  
physically precedes the other. In such a case, the two messages sent can be  
received in any order.  Similarly, if two receive operations that are <b> logically concurrent</b>  
receive two successively sent messages, then the two messages can  
match the two receives in either order.  
<P> 
 
<br> 
<em> Advice  
        to implementors.</em>  
<P> 
The <font face="sans-serif"> MPI</font> Forum believes the previous paragraph is ambiguous and may clarify the meaning in a future version of the <font face="sans-serif"> MPI</font> Standard.  
 (<em> End of advice to implementors.</em>) <br> 
<br><b> Example</b>  
  
An example of nonovertaking messages.  
<P> 
<P><img width=883 height=225 src="img15.gif" alt="Image file"><P>
The message sent by the first send must be received by the first receive, and  
the message sent by the second send must be received by the second receive.  
  
<P> 
<b> Progress.</b>  
If a pair of  
matching send and receive operations have been initiated, then at  
least one of these two operations will complete, independently of  
other actions in the system: the send operation will  
complete, unless the receive is satisfied by another message, and  
completes; the  
receive operation will complete, unless the message sent is consumed by another  
matching receive that was <em> started</em> at the same destination <font face="sans-serif"> MPI</font> process.  
<P> 
<br><b> Example</b>  
  
An example of two, intertwined matching pairs.  
<P><img width=843 height=200 src="img16.gif" alt="Image file"><P>
Both <font face="sans-serif"> MPI</font> processes invoke their first communication call.  
Since the first send at the <font face="sans-serif"> MPI</font> process with <tt>rank = 0</tt> uses the buffered mode, it must complete,  
irrespective of the state of the other <font face="sans-serif"> MPI</font> process(es).  Since no matching receive is  
<em> started</em>, the message will be copied into buffer space.  (If insufficient  
buffer space is available, then the program will fail.)  
The second send is then invoked.  
At that point, a matching pair of send and receive  
operation is enabled, and both operations must complete.  
Next, the second receive call is invoked, which will be satisfied by the buffered  
message. Note that the <font face="sans-serif"> MPI</font> process with <tt>rank = 1</tt> received the messages in the reverse order they  
were sent.  
  
<P> 
<b> Fairness.</b>  
<font face="sans-serif"> MPI</font> makes no guarantee of <b> fairness</b> in the handling of  
communication.  Suppose that a send is <em> started</em>.  Then it is possible  
that the destination <font face="sans-serif"> MPI</font> process repeatedly posts a receive that matches this  
send, yet the message is never received, because it is each time overtaken by  
another message, sent from another source.  Similarly, suppose that a  
receive was <em> started</em> by a multithreaded <font face="sans-serif"> MPI</font> process.  Then it is possible that  
messages that  
match this receive are repeatedly received, yet the receive is never satisfied,  
because it is overtaken by other receives <em> started</em> at this <font face="sans-serif"> MPI</font> process (by  
other executing threads).  It is the programmer's responsibility to prevent  
starvation in such situations.  
<P> 
<b> Resource limitations.</b>  
Any <em> pending</em> communication operation  
and <em> decoupled </em><font face="sans-serif"> MPI</font> activity</em>  
consumes system resources that are limited.  
Errors may occur when lack of resources prevent the execution of an <font face="sans-serif"> MPI</font> call.  
High-quality implementations will use a (small) fixed amount of resources for  
each <em> pending</em> send in the ready or synchronous mode and for each  
<em> pending</em> receive. However, buffer  
space may be consumed to store messages sent in standard mode, and must  
be consumed to store  
messages sent in buffered mode, when no matching receive is available.  
The amount of space available for buffering will be much smaller than program  
data memory on many systems.  Then, it will be easy to write programs that  
overrun available buffer space.  
<P> 
<font face="sans-serif"> MPI</font> allows the user to provide buffer memory for messages sent in the buffered  
mode.  Furthermore, <font face="sans-serif"> MPI</font> specifies a detailed operational  
model for the use of this  
buffer.  An <font face="sans-serif"> MPI</font> implementation is required  
to do no worse than implied by this model.  This allows users to avoid buffer  
overflows when they use buffered sends.  Buffer allocation and use is  
described  
in Section <a href="node69.htm#Node69">Buffer Allocation and Usage</a>.  
<P> 
A buffered send operation that cannot complete because of a lack of buffer space  
is <em> erroneous</em>.  When such a situation is detected, an error is signaled that may  
cause the program to terminate abnormally.  
On the other hand, a standard send  
operation that cannot complete because  
of lack of buffer space will merely block, waiting for buffer space to become  
available or for a matching receive to be <em> started</em>.  This behavior is preferable  
in many situations.  Consider a situation where a producer repeatedly produces  
new values and sends them to a consumer.  Assume that the producer produces  
new values faster than the consumer can consume them.  If buffered sends are  
used, then a buffer overflow will result.  Additional synchronization has to  
be added to the program so as to prevent this from occurring.  
If standard sends are used, then the producer will be automatically throttled,  
as its send operations will block when buffer space is unavailable.  
<P> 
In some situations, a lack of buffer space leads to deadlock situations.  
This is illustrated by the examples below.  
<P> 
<br><b> Example</b>  
  
An exchange of messages.  
<P> 
<P><img width=868 height=200 src="img17.gif" alt="Image file"><P>
This program will succeed even if no buffer space for data  
is available.  The standard send operation can be replaced, in this example,  
with a synchronous send.  
  
<P> 
<br><b> Example</b>  
  
An errant attempt to exchange messages.  
<P><img width=868 height=225 src="img18.gif" alt="Image file"><P>
The receive operation of the <font face="sans-serif"> MPI</font> process with <tt>rank = 0</tt> must complete before its send, and  
can complete only if the matching send  
of the <font face="sans-serif"> MPI</font> process with <tt>rank = 1</tt> is executed. The receive operation of the  
<font face="sans-serif"> MPI</font> process with <tt>rank = 1</tt> must complete before its send and  
can complete only if the matching send of the <font face="sans-serif"> MPI</font> process with <tt>rank = 0</tt> is executed.  
This program will always deadlock.  The same holds for any other send mode.  
  
<P> 
<br><b> Example</b>  
  
An unsafe exchange that relies on <font face="sans-serif"> MPI</font> to provide sufficient buffering.  
<P><img width=868 height=225 src="img19.gif" alt="Image file"><P>
The message sent by each <font face="sans-serif"> MPI</font> process has to be copied out before the send operation  
completes and the receive operation starts.  For the program to complete, it is  
necessary that at least one of the two messages sent be buffered.  
Thus, this program can  
succeed only if  
the communication system can buffer at least <font face="sans-serif"> count</font> words of data.  
  
<P> 
 
<br> 
<em> Advice to users.</em>  
<P> 
If standard mode send operations are used as in Example <a href="node68.htm#Node68">Semantics of Point-to-Point Communication</a>, then a deadlock situation may occur  
where both <font face="sans-serif"> MPI</font> processes are blocked because sufficient buffer space is not  
available.  
The same will certainly happen, if the synchronous mode is used.  
If the buffered mode is used, and not enough buffer space is available,  
then the  
program will not complete either.  However, rather than a deadlock situation,  
we shall have a buffer overflow error.  
<P> 
A portable program using standard mode send operations should not rely on message  
buffering for the program to complete without <em> deadlock</em>.  
All sends in such a portable program can be replaced  
with synchronous mode sends and the program will still run correctly.  
The buffered send mode can be used for programs that require buffering.  
<P> 
Nonblocking message-passing operations, as described in  
Section <a href="node71.htm#Node71">Nonblocking Communication</a>, can be used to avoid the need for buffering  
outgoing messages.  This can prevent unintentional <em> serialization</em> or <em> deadlock</em> due to lack of buffer space, and  
improves performance, by allowing <em> overlap</em> of communication with other communication or with computation,  
and avoiding the overheads of allocating buffers and copying messages into  
buffers.  
 (<em> End of advice to users.</em>) <br> 

<P>
<hr>
<a href="node67.htm#Node67"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node53.htm#Node53"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node69.htm#Node69"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node53.htm#Node53"> Point-to-Point Communication</a>
<b>Next: </b><a href="node69.htm#Node69"> Buffer Allocation and Usage</a>
<b>Previous: </b><a href="node67.htm#Node67"> Communication Modes</a>
<p>
<HR>
Return to <A HREF="node601.htm">MPI-4.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-4.1 of November 2, 2023<BR>
HTML Generated on November 19, 2023
</FONT>
</body>
</html>
