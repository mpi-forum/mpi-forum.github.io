<!DOCTYPE html>
<html lang=en>
<head>
<!-- This file was generated by tohtml from chap-topol/topol-rendered -->
<!-- with the command
tohtml -default -numbers -dosnl -htables -quietlatex -allgif -endpage mpi4-forum-tail.htm -Wnoredef --mpidoc --latexpgm pdflatex --indexfile mpi41-report-html.idx -basedef mpi4defs.txt -o mpi41-report.tex mpi-report.tex 
-->
<title>Neighborhood Gather</title>
</head>
<body style="background-color:#FFFFFF">
<hr><h2><span id="Node233">9.6.1. Neighborhood Gather</span></h2>
<a href="node232.htm#Node232"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node232.htm#Node232"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node234.htm#Node234"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node232.htm#Node232"> Neighborhood Collective Communication on Virtual Topologies</a>
<b>Next: </b><a href="node234.htm#Node234"> Neighborhood Alltoall</a>
<b>Previous: </b><a href="node232.htm#Node232"> Neighborhood Collective Communication on Virtual Topologies</a>
<p>
  
<P> 
In the neighborhood gather operation, each <font face="sans-serif"> MPI</font> process <i>i</i> gathers data items from each <font face="sans-serif"> MPI</font> process  
<i>j</i> if an edge <i>(j,i)</i> exists in the topology graph, and each  
<font face="sans-serif"> MPI</font> process <i>i</i> sends the same data items to all <font face="sans-serif"> MPI</font> processes <i>j</i> where an edge <i>(i,j)</i>  
exists. The send buffer is sent to each neighboring <font face="sans-serif"> MPI</font> process and the  
<i>l</i>-th block in the receive buffer is received from the <i>l</i>-th neighbor.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_NEIGHBOR_ALLGATHER(<span style="white-space:nowrap">sendbuf</span>, <span style="white-space:nowrap">sendcount</span>, <span style="white-space:nowrap">sendtype</span>, <span style="white-space:nowrap">recvbuf</span>, <span style="white-space:nowrap">recvcount</span>, <span style="white-space:nowrap">recvtype</span>, <span style="white-space:nowrap">comm</span>)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN sendcount</TD><TD>number of elements sent to each neighbor (non-negative integer)</TD></TR>  
<TR><TD> IN sendtype</TD><TD>datatype of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcount</TD><TD>number of elements received from each neighbor (non-negative integer)</TD></TR>  
<TR><TD> IN recvtype</TD><TD>datatype of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator with associated virtual topology (handle)</TD></TR>  
</TABLE>  
  <b> C binding</b><br>  <tt> int MPI_Neighbor_allgather(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
  
  <tt> int MPI_Neighbor_allgather_c(const void *sendbuf, MPI_Count sendcount, MPI_Datatype sendtype, void *recvbuf, MPI_Count recvcount, MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
  <b> Fortran 2008 binding</b><br>  <tt> MPI_Neighbor_allgather(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, ierror) <br><br>TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span> <br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">sendcount</span>, <span style="white-space:nowrap">recvcount</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">sendtype</span>, <span style="white-space:nowrap">recvtype</span> <br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <tt> MPI_Neighbor_allgather(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, ierror) !(_c) <br><br>TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span> <br>INTEGER(KIND=MPI_COUNT_KIND), INTENT(IN) :: <span style="white-space:nowrap">sendcount</span>, <span style="white-space:nowrap">recvcount</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">sendtype</span>, <span style="white-space:nowrap">recvtype</span> <br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <b> Fortran binding</b><br>  <tt> MPI_NEIGHBOR_ALLGATHER(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNT, RECVTYPE, COMM, IERROR) <br><br>&lt;type&gt; <span style="white-space:nowrap">SENDBUF(*)</span>, <span style="white-space:nowrap">RECVBUF(*)</span> <br>INTEGER <span style="white-space:nowrap">SENDCOUNT</span>, <span style="white-space:nowrap">SENDTYPE</span>, <span style="white-space:nowrap">RECVCOUNT</span>, <span style="white-space:nowrap">RECVTYPE</span>, <span style="white-space:nowrap">COMM</span>, <span style="white-space:nowrap">IERROR</span> <br></tt>  
<P> 
The <font face="sans-serif"> MPI_NEIGHBOR_ALLGATHER</font> procedure supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described  
in Section <a href="node232.htm#Node232">Neighborhood Collective Communication on Virtual Topologies</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as  
if each <font face="sans-serif"> MPI</font> process executed sends to each of its outgoing neighbors and  
receives from each of its incoming neighbors:  
<P> 
<P><img width=878 height=399 src="img318.gif" alt="Image file"><P>
Figure <a href="node233.htm#Node233">Neighborhood Gather</a> shows the neighborhood gather  
communication of one <font face="sans-serif"> MPI</font> process with outgoing neighbors <i>d<SUB>0</SUB>... d<SUB>3</SUB></i> and  
incoming neighbors <i>s<SUB>0</SUB>... s<SUB>5</SUB></i>. The <font face="sans-serif"> MPI</font> process will send its  
<font face="sans-serif"> sendbuf</font> to all four <font face="sans-serif"> destinations</font> (outgoing neighbors)  
and it will receive the contribution from all six <font face="sans-serif"> sources</font>  
(incoming neighbors) into separate locations of its receive buffer.  
<P> 
  <div style="text-align:center"></div><P><span id="node233.htm#Equation21"><img width=563 height=430 src="img319.gif" alt="Image file"></span><P>
  
  <br> 
Neighborhood gather communication example<P> 
  
    
All arguments are significant on all <font face="sans-serif"> MPI</font> processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all <font face="sans-serif"> MPI</font> processes.  
<P> 
The type signature associated with <font face="sans-serif"> sendcount</font>, <font face="sans-serif"> sendtype</font>  
at an <font face="sans-serif"> MPI</font> process must be equal to the type signature associated with  
<font face="sans-serif"> recvcount</font>, <font face="sans-serif"> recvtype</font> at all other <font face="sans-serif"> MPI</font> processes.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of  
communicating <font face="sans-serif"> MPI</font> processes.  
Distinct type maps between sender and receiver are still allowed.  
<P> 
 
<br> 
<em> Rationale.</em>  
<P> 
For optimization reasons, the same type signature is required  
independently of whether the topology graph is connected or not.  
 (<em> End of rationale.</em>) <br> 
The ``in place'' option is not meaningful for this operation.  
<P> 
<br><b> Example</b>  
  
Buffer usage of <font face="sans-serif"> MPI_NEIGHBOR_ALLGATHER</font> in the case of a Cartesian virtual topology.  
<P> 
On a Cartesian virtual topology, the buffer usage in a given   
direction <font face="sans-serif"> d</font> with <font face="sans-serif"> dims[d]=3</font> and <font face="sans-serif"> 1</font>,  
respectively during creation of the communicator is described   
in Figure <a href="node233.htm#Figure22">22</a>.  
<P> 
The figure may apply to any (or multiple) directions in   
the Cartesian topology. The grey buffers are required in all  
cases but are only accessed if during creation of the communicator,   
<font face="sans-serif"> periods[d]</font> was defined as nonzero (in C) or <font face="sans-serif"> .TRUE.</font> (in Fortran).  
  
<P> 
  <div style="text-align:center"><P><img width=1170 height=117 src="cart_neighbor_allgather.gif" alt="Image file"><P>
</div>  
  <br> 
<b>Figure 22: </b><span id="Figure22">Cartesian neighborhood allgather example for 3 and 1 processes in a dimension</span><P> 
  
    
The vector variant of <font face="sans-serif"> MPI_NEIGHBOR_ALLGATHER</font> allows one to gather  
different numbers of elements from each neighbor.  
<P> 
<TABLE><TR><TD COLSPAN=2>MPI_NEIGHBOR_ALLGATHERV(<span style="white-space:nowrap">sendbuf</span>, <span style="white-space:nowrap">sendcount</span>, <span style="white-space:nowrap">sendtype</span>, <span style="white-space:nowrap">recvbuf</span>, <span style="white-space:nowrap">recvcounts</span>, <span style="white-space:nowrap">displs</span>, <span style="white-space:nowrap">recvtype</span>, <span style="white-space:nowrap">comm</span>)</TD></TR>  
<TR><TD> IN sendbuf</TD><TD>starting address of send buffer (choice)</TD></TR>  
<TR><TD> IN sendcount</TD><TD>number of elements sent to each neighbor (non-negative integer)</TD></TR>  
<TR><TD> IN sendtype</TD><TD>datatype of send buffer elements (handle)</TD></TR>  
<TR><TD> OUT recvbuf</TD><TD>starting address of receive buffer (choice)</TD></TR>  
<TR><TD> IN recvcounts</TD><TD>nonnegative integer array (of length indegree) containing the number of elements that are received from each neighbor</TD></TR>  
<TR><TD> IN displs</TD><TD>integer array (of length indegree). Entry <font face="sans-serif"> i</font> specifies the displacement (relative to <font face="sans-serif"> recvbuf</font>) at which to place the incoming data from neighbor <font face="sans-serif"> i</font></TD></TR>  
<TR><TD> IN recvtype</TD><TD>datatype of receive buffer elements (handle)</TD></TR>  
<TR><TD> IN comm</TD><TD>communicator with associated virtual topology (handle)</TD></TR>  
</TABLE>  
  <b> C binding</b><br>  <tt> int MPI_Neighbor_allgatherv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, const int recvcounts[], const int displs[], MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
  
  <tt> int MPI_Neighbor_allgatherv_c(const void *sendbuf, MPI_Count sendcount, MPI_Datatype sendtype, void *recvbuf, const MPI_Count recvcounts[], const MPI_Aint displs[], MPI_Datatype recvtype, MPI_Comm comm) <br></tt>  
  <b> Fortran 2008 binding</b><br>  <tt> MPI_Neighbor_allgatherv(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, ierror) <br><br>TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span> <br>INTEGER, INTENT(IN) :: <span style="white-space:nowrap">sendcount</span>, <span style="white-space:nowrap">recvcounts(*)</span>, <span style="white-space:nowrap">displs(*)</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">sendtype</span>, <span style="white-space:nowrap">recvtype</span> <br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <tt> MPI_Neighbor_allgatherv(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, ierror) !(_c) <br><br>TYPE(*), DIMENSION(..), INTENT(IN) :: <span style="white-space:nowrap">sendbuf</span> <br>INTEGER(KIND=MPI_COUNT_KIND), INTENT(IN) :: <span style="white-space:nowrap">sendcount</span>, <span style="white-space:nowrap">recvcounts(*)</span> <br>TYPE(MPI_Datatype), INTENT(IN) :: <span style="white-space:nowrap">sendtype</span>, <span style="white-space:nowrap">recvtype</span> <br>TYPE(*), DIMENSION(..) :: <span style="white-space:nowrap">recvbuf</span> <br>INTEGER(KIND=MPI_ADDRESS_KIND), INTENT(IN) :: <span style="white-space:nowrap">displs(*)</span> <br>TYPE(MPI_Comm), INTENT(IN) :: <span style="white-space:nowrap">comm</span> <br>INTEGER, OPTIONAL, INTENT(OUT) :: <span style="white-space:nowrap">ierror</span> <br></tt>  
  <b> Fortran binding</b><br>  <tt> MPI_NEIGHBOR_ALLGATHERV(SENDBUF, SENDCOUNT, SENDTYPE, RECVBUF, RECVCOUNTS, DISPLS, RECVTYPE, COMM, IERROR) <br><br>&lt;type&gt; <span style="white-space:nowrap">SENDBUF(*)</span>, <span style="white-space:nowrap">RECVBUF(*)</span> <br>INTEGER <span style="white-space:nowrap">SENDCOUNT</span>, <span style="white-space:nowrap">SENDTYPE</span>, <span style="white-space:nowrap">RECVCOUNTS(*)</span>, <span style="white-space:nowrap">DISPLS(*)</span>, <span style="white-space:nowrap">RECVTYPE</span>, <span style="white-space:nowrap">COMM</span>, <span style="white-space:nowrap">IERROR</span> <br></tt>  
<P> 
The <font face="sans-serif"> MPI_NEIGHBOR_ALLGATHERV</font> procedure supports Cartesian communicators, graph communicators, and  
distributed graph communicators as described  
in Section <a href="node232.htm#Node232">Neighborhood Collective Communication on Virtual Topologies</a>.  
If <font face="sans-serif"> comm</font> is a distributed graph communicator, the outcome is as if each  
<font face="sans-serif"> MPI</font> process executed sends to each of its outgoing neighbors and receives from  
each of its incoming neighbors:  
<P> 
<P><img width=891 height=399 src="img320.gif" alt="Image file"><P>
The type signature associated with <font face="sans-serif"> sendcount</font>, <font face="sans-serif"> sendtype</font> at  
<font face="sans-serif"> MPI</font> process <i>j</i> must be equal to the type signature associated with  
<font face="sans-serif"> recvcounts</font><font face="sans-serif"> [l]</font>, <font face="sans-serif"> recvtype</font> at any other <font face="sans-serif"> MPI</font> process  
with <font face="sans-serif"> srcs[l]=<i>j</i></font>.  
This implies that the amount of data sent must be equal to the  
amount of data received, pairwise between every pair of  
communicating <font face="sans-serif"> MPI</font> processes.  
Distinct type maps between sender and receiver are still allowed.  
The data received from the <font face="sans-serif"> l</font>-th neighbor is placed into  
<font face="sans-serif"> recvbuf</font> beginning at offset <font face="sans-serif"> displs</font><font face="sans-serif"> [l]</font>  
elements (in terms of the <font face="sans-serif"> recvtype</font>).   
<P> 
The ``in place'' option is not meaningful for this operation.  
<P> 
All arguments are significant on all <font face="sans-serif"> MPI</font> processes and the argument  
<font face="sans-serif"> comm</font> must have identical values on all <font face="sans-serif"> MPI</font> processes.  
<P> 

<P>
<hr>
<a href="node232.htm#Node232"><img width=16 height=16 src="previous.gif" alt="Previous"></a><a href="node232.htm#Node232"><img width=16 height=16 src="up.gif" alt="Up"></a><a href="node234.htm#Node234"><img width=16 height=16 src="next.gif" alt="Next"></a><br>
<b>Up: </b><a href="node232.htm#Node232"> Neighborhood Collective Communication on Virtual Topologies</a>
<b>Next: </b><a href="node234.htm#Node234"> Neighborhood Alltoall</a>
<b>Previous: </b><a href="node232.htm#Node232"> Neighborhood Collective Communication on Virtual Topologies</a>
<p>
<HR>
Return to <A HREF="node601.htm">MPI-4.1 Standard Index</A><BR>
Return to <A HREF="http://www.mpi-forum.org/index.html">MPI Forum Home Page</A><BR>
<HR>
<FONT SIZE=-1>(Unofficial) MPI-4.1 of November 2, 2023<BR>
HTML Generated on November 19, 2023
</FONT>
</body>
</html>
